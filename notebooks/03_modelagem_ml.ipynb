{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cab2e467",
   "metadata": {},
   "source": [
    "---\n",
    "# Algoritmos para Big Data – Projeto\n",
    "## Parte 3: Modelação e Previsão\n",
    "\n",
    "**Dataset:** Flight Delay Dataset (amostrado e limpo)\n",
    "\n",
    "**Autores:**\n",
    "- Henrique Niza (131898)\n",
    "- Paulo Francisco Pinto (128962)\n",
    "- Rute Roque (128919)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a91add3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação de bibliotecas necessárias para Spark e visualização\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a982a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/05/24 22:24:05 WARN Utils: Your hostname, MacBook-Pro-de-admin.local, resolves to a loopback address: 127.0.2.3; using 192.168.68.50 instead (on interface en0)\n",
      "25/05/24 22:24:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/24 22:24:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Criação da SparkSession\n",
    "# spark = SparkSession.builder.appName(\"FlightDelayModeling\").getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FlightDelayModeling\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e87534ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho para os dados limpos\n",
    "input_path = \"../data/processed/flights_cleaned_sample.parquet\"\n",
    "data = spark.read.parquet(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac289e6",
   "metadata": {},
   "source": [
    "---\n",
    "#### Seleção de Features para Modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "574f58aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variável-alvo: DepDel15 (1 se atraso > 15 min, 0 caso contrário)\n",
    "# Features selecionadas (numéricas e categóricas relevantes)\n",
    "features = [\n",
    "    \"Month\", \"DayofMonth\", \"DayOfWeek\", \"CRSDepTime\", \"Distance\",\n",
    "    \"DepTimeBlk\", \"Airline\", \"Origin\", \"Dest\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d17ad585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexação de variáveis categóricas\n",
    "cat_features = [\"DepTimeBlk\", \"Airline\", \"Origin\", \"Dest\"]\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c+\"_Idx\") for c in cat_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e27e0866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembler para features\n",
    "input_features = [\"Month\", \"DayofMonth\", \"DayOfWeek\", \"CRSDepTime\", \"Distance\"] + [c+\"_Idx\" for c in cat_features]\n",
    "assembler = VectorAssembler(inputCols=input_features, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8412266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexador do label\n",
    "label_indexer = StringIndexer(inputCol=\"DepDel15\", outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15a41192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classificador\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=20, seed=42, maxBins=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d45deff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "pipeline = Pipeline(stages=indexers + [label_indexer, assembler, rf])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0940ed0",
   "metadata": {},
   "source": [
    " ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "745172c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split de Dados \n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3c1c5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/24 22:24:25 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/05/24 22:25:49 WARN MemoryStore: Not enough space to cache rdd_79_1 in memory! (computed 93.8 MiB so far)\n",
      "25/05/24 22:25:49 WARN BlockManager: Persisting block rdd_79_1 to disk instead.\n",
      "25/05/24 22:25:49 WARN MemoryStore: Not enough space to cache rdd_79_2 in memory! (computed 93.8 MiB so far)\n",
      "25/05/24 22:25:49 WARN BlockManager: Persisting block rdd_79_2 to disk instead.\n",
      "25/05/24 22:25:51 WARN MemoryStore: Not enough space to cache rdd_79_0 in memory! (computed 140.7 MiB so far)\n",
      "25/05/24 22:25:51 WARN BlockManager: Persisting block rdd_79_0 to disk instead.\n",
      "25/05/24 22:25:51 WARN MemoryStore: Not enough space to cache rdd_79_7 in memory! (computed 140.7 MiB so far)\n",
      "25/05/24 22:25:51 WARN BlockManager: Persisting block rdd_79_7 to disk instead.\n",
      "25/05/24 22:25:51 WARN MemoryStore: Not enough space to cache rdd_79_3 in memory! (computed 140.7 MiB so far)\n",
      "25/05/24 22:25:51 WARN BlockManager: Persisting block rdd_79_3 to disk instead.\n",
      "25/05/24 22:25:51 WARN MemoryStore: Not enough space to cache rdd_79_5 in memory! (computed 140.7 MiB so far)\n",
      "25/05/24 22:25:51 WARN BlockManager: Persisting block rdd_79_5 to disk instead.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Treinamento do Modelo\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3de937e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previsões e Avaliação\n",
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "783b5e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do modelo RandomForest: 0.8274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Avaliação da acurácia\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Acurácia do modelo RandomForest: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cfd5f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:==============>                                           (2 + 6) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------+\n",
      "|label|prediction|  count|\n",
      "+-----+----------+-------+\n",
      "|  0.0|       0.0|1406579|\n",
      "|  1.0|       0.0| 293409|\n",
      "+-----+----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Matriz de Confusão\n",
    "confusion = predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\")\n",
    "confusion.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8568937a",
   "metadata": {},
   "source": [
    "Fica assim preparado para ser continuado com tuning, outras métricas ou modelos adicionais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b6e0036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo salvo com sucesso em: ../models/flight_delay_rf_model\n"
     ]
    }
   ],
   "source": [
    "# Guardar o modelo treinado\n",
    "model_path = \"../models/flight_delay_rf_model\"\n",
    "model.write().overwrite().save(model_path)\n",
    "print(f\"Modelo salvo com sucesso em: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d436e4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do modelo: 0.8274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Avaliação do modelo\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "predictions = model.transform(test_data)\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Acurácia do modelo: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7b91bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 71:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|[1.0,1.0,1.0,2200...|  1.0|       0.0|\n",
      "|[1.0,1.0,1.0,1747...|  0.0|       0.0|\n",
      "|[1.0,1.0,1.0,1200...|  0.0|       0.0|\n",
      "|[1.0,1.0,1.0,2040...|  0.0|       0.0|\n",
      "|[1.0,1.0,1.0,1000...|  0.0|       0.0|\n",
      "|[1.0,1.0,1.0,1600...|  0.0|       0.0|\n",
      "|[1.0,1.0,1.0,1510...|  0.0|       0.0|\n",
      "|[1.0,1.0,1.0,1035...|  0.0|       0.0|\n",
      "|[1.0,1.0,1.0,1126...|  0.0|       0.0|\n",
      "|[1.0,1.0,1.0,2230...|  1.0|       0.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.select(\"features\", \"label\", \"prediction\").show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

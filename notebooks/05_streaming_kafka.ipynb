{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efbc0f4d",
   "metadata": {},
   "source": [
    "---\n",
    "# Algoritmos para Big Data – Projeto\n",
    "## Parte 5: Previsão de Atrasos em Streaming com Kafka\n",
    "\n",
    "**Dataset:** Previsões em tempo real simuladas\n",
    "\n",
    "**Autores:**\n",
    "- Henrique Niza (131898)\n",
    "- Paulo Francisco Pinto (128962)\n",
    "- Rute Roque (128919)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "891f621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Spark Setup\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType, IntegerType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FlightDelayStreamingKafka\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48f90fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo carregado de: ../models/flight_delay_rf_model\n"
     ]
    }
   ],
   "source": [
    "# 2. Carregar modelo treinado\n",
    "model_path = \"../models/flight_delay_rf_model\"\n",
    "model = PipelineModel.load(model_path)\n",
    "\n",
    "print(f\"Modelo carregado de: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf7ca4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "    .add(\"Airline\", StringType()) \\\n",
    "    .add(\"Origin\", StringType()) \\\n",
    "    .add(\"Dest\", StringType()) \\\n",
    "    .add(\"CRSDepTime\", IntegerType()) \\\n",
    "    .add(\"Distance\", DoubleType()) \\\n",
    "    .add(\"DepDelay\", DoubleType()) \\\n",
    "    .add(\"Month\", IntegerType()) \\\n",
    "    .add(\"DepTimeBlk\", StringType()) \\\n",
    "    .add(\"DepDel15\", IntegerType()) \\\n",
    "    .add(\"DayofMonth\", IntegerType()) \\\n",
    "    .add(\"DayOfWeek\", IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ff90b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Leitura do stream Kafka\n",
    "kafka_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"flights\") \\\n",
    "    .option(\"startingOffsets\", \"latest\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a98a569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Parse das mensagens JSON\n",
    "flights_df = kafka_stream.selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "    .select(from_json(col(\"json\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e532795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mensagem enviada!\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "msg = {\n",
    "    \"Airline\": \"Delta Air Lines Inc.\",\n",
    "    \"Origin\": \"ATL\",\n",
    "    \"Dest\": \"JFK\",\n",
    "    \"CRSDepTime\": 830,\n",
    "    \"Distance\": 760.0,\n",
    "    \"DepDelay\": 5.0,\n",
    "    \"Month\": 5,\n",
    "    \"DepTimeBlk\": \"0800-0859\",\n",
    "    \"DepDel15\": 0,\n",
    "    \"DayofMonth\": 24,\n",
    "    \"DayOfWeek\": 6 \n",
    "}\n",
    "\n",
    "producer.send('flights', msg)\n",
    "producer.flush()\n",
    "print(\"Mensagem enviada!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba40cff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df = kafka_stream.selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "    .select(from_json(col(\"json\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95352f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Aplicar o modelo treinado para gerar previsões\n",
    "predicoes = model.transform(flights_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a8065944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/24 23:45:34 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/zn/qww_x74n5_d1b332p060685c0000gn/T/temporary-40b430f6-a6ba-4c23-b595-e02cfa5da626. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/24 23:45:34 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/24 23:45:34 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+------+----+--------+----------+\n",
      "|Airline|Origin|Dest|DepDelay|prediction|\n",
      "+-------+------+----+--------+----------+\n",
      "+-------+------+----+--------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Escrever as previsões para o console (apenas para debug)\n",
    "query = predicoes.select(\"Airline\", \"Origin\", \"Dest\", \"DepDelay\", \"prediction\") \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination(300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
